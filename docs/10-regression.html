<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Statistical Methodology</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Introduction to Statistical Methodology">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Statistical Methodology" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/STA_570_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Statistical Methodology" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2017-04-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="9-analysis-of-variance-anova.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://dereksonderegger.github.io/570/Statistical_Methods_I.pdf" target="blank">PDF version</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html"><i class="fa fa-check"></i><b>1</b> Summary Statistics and Graphing</a><ul>
<li class="chapter" data-level="1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#graphical-summaries-of-data"><i class="fa fa-check"></i><b>1.1</b> Graphical summaries of data</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#univariate---categorical"><i class="fa fa-check"></i><b>1.1.1</b> Univariate - Categorical</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#univariate---continuous"><i class="fa fa-check"></i><b>1.1.2</b> Univariate - Continuous</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#bivariate---categorical-vs-continuous"><i class="fa fa-check"></i><b>1.1.3</b> Bivariate - Categorical vs Continuous</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#bivariate---continuous-vs-continuous"><i class="fa fa-check"></i><b>1.1.4</b> Bivariate - Continuous vs Continuous</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-centrality"><i class="fa fa-check"></i><b>1.2</b> Measures of Centrality</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mean"><i class="fa fa-check"></i><b>1.2.1</b> Mean</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#median"><i class="fa fa-check"></i><b>1.2.2</b> Median</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#mode"><i class="fa fa-check"></i><b>1.2.3</b> Mode</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#examples"><i class="fa fa-check"></i><b>1.2.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#measures-of-variation"><i class="fa fa-check"></i><b>1.3</b> Measures of Variation</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#range"><i class="fa fa-check"></i><b>1.3.1</b> Range</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#inter-quartile-range"><i class="fa fa-check"></i><b>1.3.2</b> Inter-Quartile Range</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#variance"><i class="fa fa-check"></i><b>1.3.3</b> Variance</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#standard-deviation"><i class="fa fa-check"></i><b>1.3.4</b> Standard Deviation</a></li>
<li class="chapter" data-level="1.3.5" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#coefficient-of-variation"><i class="fa fa-check"></i><b>1.3.5</b> Coefficient of Variation</a></li>
<li class="chapter" data-level="1.3.6" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#empirical-rule-of-thumb"><i class="fa fa-check"></i><b>1.3.6</b> Empirical Rule of Thumb</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-summary-statistics-and-graphing.html"><a href="1-summary-statistics-and-graphing.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-probability.html"><a href="2-probability.html"><i class="fa fa-check"></i><b>2</b> Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-set-theory"><i class="fa fa-check"></i><b>2.1</b> Introduction to Set Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-probability.html"><a href="2-probability.html#composition-of-events"><i class="fa fa-check"></i><b>2.1.1</b> Composition of events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-probability.html"><a href="2-probability.html#probability-rules"><i class="fa fa-check"></i><b>2.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-probability.html"><a href="2-probability.html#simple-rules"><i class="fa fa-check"></i><b>2.2.1</b> Simple Rules</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-probability.html"><a href="2-probability.html#conditional-probability"><i class="fa fa-check"></i><b>2.2.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-probability.html"><a href="2-probability.html#summary-of-probability-rules"><i class="fa fa-check"></i><b>2.2.3</b> Summary of Probability Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-probability.html"><a href="2-probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> Discrete Random Variables</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-probability.html"><a href="2-probability.html#introduction-to-discrete-random-variables"><i class="fa fa-check"></i><b>2.3.1</b> Introduction to Discrete Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-probability.html"><a href="2-probability.html#common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Common Discrete Distributions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-probability.html"><a href="2-probability.html#binomial-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Binomial Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-probability.html"><a href="2-probability.html#poisson-distribution"><i class="fa fa-check"></i><b>2.4.2</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-probability.html"><a href="2-probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-probability.html"><a href="2-probability.html#uniform01-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-probability.html"><a href="2-probability.html#exponential-distribution"><i class="fa fa-check"></i><b>2.5.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-probability.html"><a href="2-probability.html#normal-distribution"><i class="fa fa-check"></i><b>2.5.3</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.5.4" data-path="2-probability.html"><a href="2-probability.html#standardizing"><i class="fa fa-check"></i><b>2.5.4</b> Standardizing</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-probability.html"><a href="2-probability.html#exercises-1"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html"><i class="fa fa-check"></i><b>3</b> Confidence Intervals via Bootstrapping</a><ul>
<li class="chapter" data-level="3.1" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#theory-of-bootstrapping"><i class="fa fa-check"></i><b>3.1</b> Theory of Bootstrapping</a></li>
<li class="chapter" data-level="3.2" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#quantile-based-confidence-intervals"><i class="fa fa-check"></i><b>3.2</b> Quantile-based Confidence Intervals</a></li>
<li class="chapter" data-level="3.3" data-path="3-confidence-intervals-via-bootstrapping.html"><a href="3-confidence-intervals-via-bootstrapping.html#exercises-2"><i class="fa fa-check"></i><b>3.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html"><i class="fa fa-check"></i><b>4</b> Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></a><ul>
<li class="chapter" data-level="4.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#enlightening-example"><i class="fa fa-check"></i><b>4.1</b> Enlightening Example</a></li>
<li class="chapter" data-level="4.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mathematical-details"><i class="fa fa-check"></i><b>4.2</b> Mathematical details</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#probability-rules-for-expectations-and-variances"><i class="fa fa-check"></i><b>4.2.1</b> Probability Rules for Expectations and Variances</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#mean-and-variance-of-the-sample-mean"><i class="fa fa-check"></i><b>4.2.2</b> Mean and Variance of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#distribution-of-barx"><i class="fa fa-check"></i><b>4.3</b> Distribution of <span class="math inline">\(\bar{X}\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#central-limit-theorem"><i class="fa fa-check"></i><b>4.4</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="4.5" data-path="4-sampling-distribution-of-barx.html"><a href="4-sampling-distribution-of-barx.html#exercises-3"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals for <span class="math inline">\(\mu\)</span></a><ul>
<li class="chapter" data-level="5.1" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotic-result-sigma-known"><i class="fa fa-check"></i><b>5.1</b> Asymptotic result (<span class="math inline">\(\sigma\)</span> known)</a></li>
<li class="chapter" data-level="5.2" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#asymptotoic-result-sigma-unknown"><i class="fa fa-check"></i><b>5.2</b> Asymptotoic result (<span class="math inline">\(\sigma\)</span> unknown)</a></li>
<li class="chapter" data-level="5.3" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#sample-size-selection"><i class="fa fa-check"></i><b>5.3</b> Sample Size Selection</a></li>
<li class="chapter" data-level="5.4" data-path="5-confidence-intervals-for-mu.html"><a href="5-confidence-intervals-for-mu.html#exercises-4"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html"><i class="fa fa-check"></i><b>6</b> Hypothesis Tests for the mean of a population</a><ul>
<li class="chapter" data-level="6.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#writing-hypotheses"><i class="fa fa-check"></i><b>6.1</b> Writing Hypotheses</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>6.1.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#error"><i class="fa fa-check"></i><b>6.1.2</b> Error</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#why-should-hypotheses-use-mu-and-not-barx"><i class="fa fa-check"></i><b>6.1.3</b> Why should hypotheses use <span class="math inline">\(\mu\)</span> and not <span class="math inline">\(\bar{x}\)</span>?</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#calculating-p-values"><i class="fa fa-check"></i><b>6.1.4</b> Calculating p-values</a></li>
<li class="chapter" data-level="6.1.5" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#calculating-p-values-vs-cutoff-values"><i class="fa fa-check"></i><b>6.1.5</b> Calculating p-values vs cutoff values</a></li>
<li class="chapter" data-level="6.1.6" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#t-tests-in-r"><i class="fa fa-check"></i><b>6.1.6</b> t-tests in R</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i><b>6.2</b> Type I and Type II Errors</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#power-and-sample-size-selection"><i class="fa fa-check"></i><b>6.2.1</b> Power and Sample Size Selection</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-hypothesis-tests-for-the-mean-of-a-population.html"><a href="6-hypothesis-tests-for-the-mean-of-a-population.html#exercises-5"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><i class="fa fa-check"></i><b>7</b> Two-Sample Hypothesis Tests and Confidence Intervals</a><ul>
<li class="chapter" data-level="7.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups"><i class="fa fa-check"></i><b>7.1</b> Difference in means between two groups</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-resampling"><i class="fa fa-check"></i><b>7.1.1</b> Inference via resampling</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-unequal-variance-assumption"><i class="fa fa-check"></i><b>7.1.2</b> Inference via asymptotic results (unequal variance assumption)</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#inference-via-asymptotic-results-equal-variance-assumption"><i class="fa fa-check"></i><b>7.1.3</b> Inference via asymptotic results (equal variance assumption)</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#difference-in-means-between-two-groups-paired-data"><i class="fa fa-check"></i><b>7.2</b> Difference in means between two groups: Paired Data</a></li>
<li class="chapter" data-level="7.3" data-path="7-two-sample-hypothesis-tests-and-confidence-intervals.html"><a href="7-two-sample-hypothesis-tests-and-confidence-intervals.html#exercises-6"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html"><i class="fa fa-check"></i><b>8</b> Testing Model Assumptions</a><ul>
<li class="chapter" data-level="8.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#testing-normality"><i class="fa fa-check"></i><b>8.1</b> Testing Normality</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#visual-inspection---qqplots"><i class="fa fa-check"></i><b>8.1.1</b> Visual Inspection - QQplots</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#tests-for-normality"><i class="fa fa-check"></i><b>8.1.2</b> Tests for Normality</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#testing-equal-variance"><i class="fa fa-check"></i><b>8.2</b> Testing Equal Variance</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#visual-inspection"><i class="fa fa-check"></i><b>8.2.1</b> Visual Inspection</a></li>
<li class="chapter" data-level="8.2.2" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#tests-for-equal-variance"><i class="fa fa-check"></i><b>8.2.2</b> Tests for Equal Variance</a></li>
<li class="chapter" data-level="8.2.3" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#symmetry-of-the-f-distribution"><i class="fa fa-check"></i><b>8.2.3</b> Symmetry of the F-distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#power-of-the-f-test"><i class="fa fa-check"></i><b>8.3</b> Power of the F-test</a></li>
<li class="chapter" data-level="8.4" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#theoretical-distribution-vs-bootstrap"><i class="fa fa-check"></i><b>8.4</b> Theoretical distribution vs bootstrap</a></li>
<li class="chapter" data-level="8.5" data-path="8-testing-model-assumptions.html"><a href="8-testing-model-assumptions.html#exercises-7"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>9</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="9.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#model"><i class="fa fa-check"></i><b>9.1</b> Model</a></li>
<li class="chapter" data-level="9.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#theory"><i class="fa fa-check"></i><b>9.2</b> Theory</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-table"><i class="fa fa-check"></i><b>9.2.1</b> Anova Table</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-using-simple-vs-complex-models."><i class="fa fa-check"></i><b>9.2.2</b> ANOVA using Simple vs Complex models.</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#parameter-estimates-and-confidence-intervals"><i class="fa fa-check"></i><b>9.2.3</b> Parameter Estimates and Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#anova-in-r"><i class="fa fa-check"></i><b>9.3</b> Anova in R</a></li>
<li class="chapter" data-level="9.4" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#multiple-comparisons"><i class="fa fa-check"></i><b>9.4</b> Multiple comparisons</a></li>
<li class="chapter" data-level="9.5" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#different-model-representations"><i class="fa fa-check"></i><b>9.5</b> Different Model Representations</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#theory-1"><i class="fa fa-check"></i><b>9.5.1</b> Theory</a></li>
<li class="chapter" data-level="9.5.2" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#model-representations-in-r"><i class="fa fa-check"></i><b>9.5.2</b> Model Representations in R</a></li>
<li class="chapter" data-level="9.5.3" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#implications-on-the-anova-table"><i class="fa fa-check"></i><b>9.5.3</b> Implications on the ANOVA table</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9-analysis-of-variance-anova.html"><a href="9-analysis-of-variance-anova.html#exercises-8"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-regression.html"><a href="10-regression.html"><i class="fa fa-check"></i><b>10</b> Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="10-regression.html"><a href="10-regression.html#pearsons-correlation-coefficient"><i class="fa fa-check"></i><b>10.1</b> Pearson’s Correlation Coefficient</a></li>
<li class="chapter" data-level="10.2" data-path="10-regression.html"><a href="10-regression.html#model-theory"><i class="fa fa-check"></i><b>10.2</b> Model Theory</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10-regression.html"><a href="10-regression.html#anova-interpretation"><i class="fa fa-check"></i><b>10.2.1</b> Anova Interpretation</a></li>
<li class="chapter" data-level="10.2.2" data-path="10-regression.html"><a href="10-regression.html#confidence-intervals-vs-prediction-intervals"><i class="fa fa-check"></i><b>10.2.2</b> Confidence Intervals vs Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-regression.html"><a href="10-regression.html#extrapolation"><i class="fa fa-check"></i><b>10.3</b> Extrapolation</a></li>
<li class="chapter" data-level="10.4" data-path="10-regression.html"><a href="10-regression.html#checking-model-assumptions"><i class="fa fa-check"></i><b>10.4</b> Checking Model Assumptions</a></li>
<li class="chapter" data-level="10.5" data-path="10-regression.html"><a href="10-regression.html#common-problems"><i class="fa fa-check"></i><b>10.5</b> Common Problems</a><ul>
<li class="chapter" data-level="10.5.1" data-path="10-regression.html"><a href="10-regression.html#influential-points"><i class="fa fa-check"></i><b>10.5.1</b> Influential Points</a></li>
<li class="chapter" data-level="10.5.2" data-path="10-regression.html"><a href="10-regression.html#transformations"><i class="fa fa-check"></i><b>10.5.2</b> Transformations</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="10-regression.html"><a href="10-regression.html#exercises-9"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
<li class="chapter" data-level="10.7" data-path="10-regression.html"><a href="10-regression.html#contingency-tables"><i class="fa fa-check"></i><b>10.7</b> Contingency Tables</a></li>
<li class="chapter" data-level="10.8" data-path="10-regression.html"><a href="10-regression.html#expected-counts"><i class="fa fa-check"></i><b>10.8</b> Expected Counts</a><ul>
<li class="chapter" data-level="10.8.1" data-path="10-regression.html"><a href="10-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>10.8.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="10.8.2" data-path="10-regression.html"><a href="10-regression.html#rxc-tables"><i class="fa fa-check"></i><b>10.8.2</b> RxC tables</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="10-regression.html"><a href="10-regression.html#exercises-10"><i class="fa fa-check"></i><b>10.9</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Methodology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Regression</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(ggfortify)  <span class="co"># for diagnostic plots in ggplot2 via autoplot()</span></code></pre></div>
<p>We continue to want to examine the relationship between a predictor variable and a response but now we consider the case that the predictor is continuous and the response is also continuous. In general we are going to be interested in finding the line that best fits the observed data and determining if we should include the predictor variable in the model.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-230-1.png" width="672" /></p>
<div id="pearsons-correlation-coefficient" class="section level2">
<h2><span class="header-section-number">10.1</span> Pearson’s Correlation Coefficient</h2>
<p>We first consider Pearson’s correlation coefficient, which is a statistics that measures the strength of the linear relationship between the predictor and response. Consider the following Pearson’s correlation statistic <span class="math display">\[r=\frac{\sum_{i=1}^{n}\left(\frac{x_{i}-\bar{x}}{s_{x}}\right)\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)}{n-1}\]</span> where <span class="math inline">\(x_{i}\)</span> and <span class="math inline">\(y_{i}\)</span> are the x and y coordinate of the <span class="math inline">\(i\)</span>th observation. Notice that each parenthesis value is the standardized value of each observation. If the x-value is big (greater than <span class="math inline">\(\bar{x}\)</span>) and the y-value is large (greater than <span class="math inline">\(\bar{y}\)</span>), then after multiplication, the result is positive. Likewise if the x-value is small and the y-value is small, both standardized values are negative and therefore after multiplication the result is positive. If a large x-value is paired with a small y-value, then the first value is positive, but the second is negative and so the multiplication result is negative.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-231-1.png" width="672" /></p>
<p>The following are true about Pearson’s correlation coefficient: 1. <span class="math inline">\(r\)</span> is unit-less because we have standardized the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. 2. <span class="math inline">\(-1\le r\le1\)</span> because of the scaling by <span class="math inline">\(n-1\)</span> 3. A negative <span class="math inline">\(r\)</span> denotes a negative relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, while a positive value of <span class="math inline">\(r\)</span> represents a positive relationship. 4. <span class="math inline">\(r\)</span> measures the strength of the linear relationship between the predictor and response.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-232-1.png" width="672" /></p>
</div>
<div id="model-theory" class="section level2">
<h2><span class="header-section-number">10.2</span> Model Theory</h2>
<p>To scatterplot data that looks linear we often want to fit the model <span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\;\;\;\textrm{where }\epsilon_{i}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)\]</span> where</p>
<table>
<colgroup>
<col width="23%" />
<col width="22%" />
<col width="54%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Name</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_0\)</span></td>
<td align="center">y-intercept</td>
<td align="left">Height of regression line at <span class="math inline">\(x=0\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta_1\)</span></td>
<td align="center">slope</td>
<td align="left">How much the line rises for a <span class="math inline">\(1\)</span> unit increase in <span class="math inline">\(x\)</span>.</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\sigma\)</span></td>
<td align="center">Standard Deviation</td>
<td align="left">The “typical” distance from a point to the regression line</td>
</tr>
</tbody>
</table>
<p>The assumptions of this model are:</p>
<ol style="list-style-type: decimal">
<li><em>The relationship between the predictor and response is actually linear.</em></li>
<li><em>The error terms come from a normal distribution.</em></li>
<li><em>The variance of the errors is the same for every value of x (homoscedasticity).</em></li>
<li><em>The error terms are independent.</em></li>
</ol>
<p>Under this model, the expected value of an observation with covariate <span class="math inline">\(X=x\)</span> is <span class="math inline">\(E\left(Y\,|\,X=x\right)=\beta_{0}+\beta_{1}\)</span> and a new observation has a standard deviation of <span class="math inline">\(\sigma\)</span> about the line.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-233-1.png" width="672" /></p>
<p>Given this model, how do we find estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>? In the past we have always relied on using some sort of sample mean, but it is not obvious what we can use here. Instead of a mean, we will use the values of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> that minimize the sum of squared error (SSE) where <span class="math display">\[\begin{aligned}
\hat{y}_{i} &amp;=  \hat{\beta}_{0}+\hat{\beta}_{1}x_{i} \\
e_{i}         &amp;=    y_{i}-\hat{y}_{i} \\
SSE         &amp;=  \sum_{i=1}^{n}e_{i}^{2}
\end{aligned}\]</span></p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-234-1.png" width="672" /></p>
<p>Fortunately there are simple closed form solutions for <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> <span class="math display">\[\begin{aligned}
\hat{\beta}_{1} &amp;=  r\,\left(\frac{s_{y}}{s_{x}}\right)\\
\hat{\beta_{0}} &amp;=  \bar{y}-\hat{\beta}_{1}\bar{x} 
\end{aligned}\]</span></p>
<p>and using these estimates several properties can be shown</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are the intercept and slope values that minimize SSE.</li>
<li>The regression line goes through the center of mass of the data (<span class="math inline">\(\bar{x}\)</span>,<span class="math inline">\(\bar{y}\)</span>).</li>
<li>The sum of the residuals is 0. That is: <span class="math inline">\(\sum e_{i}=0\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are unbiased estimators of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>.</li>
</ol>
<p>We are also interested in an estimate of <span class="math inline">\(\sigma^{2}\)</span> and we will use our usual estimation scheme of <span class="math display">\[\begin{aligned} \hat{\sigma}^{2}  
  &amp;= \frac{1}{n-2}\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}  
    =   \frac{\sum_{i=1}^{n}e_{i}^{2}}{n-2} 
    =   \frac{SSE}{n-2} 
    =   MSE 
    \end{aligned}\]</span></p>
<p>where the <span class="math inline">\(-2\)</span> comes from having to estimate <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> before we can estimate <span class="math inline">\(\sigma^{2}\)</span>. As in the ANOVA case, we can interpret <span class="math inline">\(\sigma\)</span> as the typical distance an observation is from its predicted value.</p>
<p>As always we are also interested in knowing the estimated standard deviation (which we will call Standard Error) of the model parameters <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> and it can be shown that <span class="math display">\[StdErr\left(\hat{\beta}_{0}\right)=\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\bar{x}^{2}}{S_{xx}}}\]</span> and <span class="math display">\[StdErr\left(\hat{\beta}_{1}\right)=\hat{\sigma}\sqrt{\frac{1}{S_{xx}}}\]</span> where <span class="math inline">\(S_{xx}=\sum\left(x_{i}-\bar{x}\right)^{2}\)</span>. These intervals can be used to calculate confidence intervals for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> using the formulas: <span class="math display">\[\hat{\beta}_{i}\pm t_{n-2}^{1-\alpha/2}StdErr\left(\hat{\beta}_{i}\right)\]</span></p>
<p>Again we consider the iris dataset that is available in R. I wish to examine the relationship between sepal length and sepal width in the species <em>setosa</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">setosa &lt;-<span class="st"> </span>iris %&gt;%<span class="st"> </span><span class="kw">filter</span>( Species ==<span class="st"> &#39;setosa&#39;</span> )    <span class="co"># Just setosa!</span>
<span class="kw">ggplot</span>(setosa, <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>Sepal.Width)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;Sepal Length&quot;</span>, <span class="dt">y=</span><span class="st">&quot;Sepal Width&quot;</span>, <span class="dt">title=</span><span class="st">&#39;Setosa Irises&#39;</span>) </code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-235-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Do all the crazy calculations &quot;By Hand!&quot;</span>
x &lt;-<span class="st"> </span>setosa$Sepal.Length
y &lt;-<span class="st"> </span>setosa$Sepal.Width
n &lt;-<span class="st"> </span><span class="kw">length</span>(x)
r &lt;-<span class="st"> </span><span class="kw">sum</span>( (x-<span class="kw">mean</span>(x))/<span class="kw">sd</span>(x) *<span class="st"> </span>(y-<span class="kw">mean</span>(y))/<span class="kw">sd</span>(y) ) /<span class="st"> </span>(n<span class="dv">-1</span>)
b1 &lt;-<span class="st"> </span>r*<span class="kw">sd</span>(y)/<span class="kw">sd</span>(x)
b0 &lt;-<span class="st"> </span><span class="kw">mean</span>(y) -<span class="st"> </span>b1*<span class="kw">mean</span>(x)
<span class="kw">cbind</span>(r, b0, b1)</code></pre></div>
<pre><code>##              r         b0        b1
## [1,] 0.7425467 -0.5694327 0.7985283</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yhat &lt;-<span class="st"> </span>b0 +<span class="st"> </span>b1*x
resid &lt;-<span class="st"> </span>y -<span class="st"> </span>yhat
SSE &lt;-<span class="st"> </span><span class="kw">sum</span>( resid^<span class="dv">2</span> )
s2 &lt;-<span class="st"> </span>SSE/(n<span class="dv">-2</span>)
s2</code></pre></div>
<pre><code>## [1] 0.06580573</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sxx &lt;-<span class="st"> </span><span class="kw">sum</span>( (x-<span class="kw">mean</span>(x))^<span class="dv">2</span> )
stderr.b0 &lt;-<span class="st"> </span><span class="kw">sqrt</span>(s2) *<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span>/n +<span class="st"> </span><span class="kw">mean</span>(x)^<span class="dv">2</span> /<span class="st"> </span>Sxx)
stderr.b1 &lt;-<span class="st"> </span><span class="kw">sqrt</span>(s2) *<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> /<span class="st"> </span>Sxx )
<span class="kw">cbind</span>(stderr.b0, stderr.b1)</code></pre></div>
<pre><code>##      stderr.b0 stderr.b1
## [1,] 0.5217119 0.1039651</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t.star &lt;-<span class="st"> </span><span class="kw">qt</span>(.<span class="dv">975</span>, <span class="dt">df=</span>n<span class="dv">-2</span>)  
<span class="kw">c</span>(b0-t.star*stderr.b0, b0+t.star*stderr.b0)</code></pre></div>
<pre><code>## [1] -1.6184048  0.4795395</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">c</span>(b1-t.star*stderr.b1, b1+t.star*stderr.b1)</code></pre></div>
<pre><code>## [1] 0.5894925 1.0075641</code></pre>
<p>Of course, we don’t want to have to do these calculations by hand. Fortunately statistics packages will do all of the above calculations. In R, we will use <code>lm()</code> to fit a linear regression model and then call various accessor functions to give me the regression output I want.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>( setosa$Sepal.Width,  setosa$Sepal.Length )</code></pre></div>
<pre><code>## [1] 0.7425467</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width ~<span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)
<span class="kw">coef</span>(model)</code></pre></div>
<pre><code>##  (Intercept) Sepal.Length 
##   -0.5694327    0.7985283</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(model)</code></pre></div>
<pre><code>##                   2.5 %    97.5 %
## (Intercept)  -1.6184048 0.4795395
## Sepal.Length  0.5894925 1.0075641</code></pre>
<p>In general, most statistics programs will give a table of output summarizing a regression and the table is usually set up as follows:</p>
<table>
<colgroup>
<col width="11%" />
<col width="12%" />
<col width="16%" />
<col width="36%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Coefficient</th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t-stat</th>
<th align="center">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="center"><span class="math inline">\(\hat{\beta}_{0}\)</span></td>
<td align="center">StdErr<span class="math inline">\((\hat{\beta}_0)\)</span></td>
<td align="center"><span class="math inline">\(t_{0}=\frac{\hat{\beta}_0}{StdErr(\hat{\beta}_0)}\)</span></td>
<td align="center"><span class="math inline">\(2*P(T_{n-2}&gt;\vert t_0 \vert )\)</span></td>
</tr>
<tr class="even">
<td align="center">Slope</td>
<td align="center"><span class="math inline">\(\hat{\beta}_{1}\)</span></td>
<td align="center">StdErr<span class="math inline">\((\hat{\beta}_1)\)</span></td>
<td align="center"><span class="math inline">\(t_{1}=\frac{\hat{\beta}_1}{StdErr(\hat{\beta}_1)}\)</span></td>
<td align="center"><span class="math inline">\(2*P(T_{n-2}&gt;\vert t_1 \vert )\)</span></td>
</tr>
</tbody>
</table>
<p>This table is printed by R by using the <code>summary()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width ~<span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)
<span class="kw">summary</span>(model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sepal.Width ~ Sepal.Length, data = setosa)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.72394 -0.18273 -0.00306  0.15738  0.51709 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -0.5694     0.5217  -1.091    0.281    
## Sepal.Length   0.7985     0.1040   7.681 6.71e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2565 on 48 degrees of freedom
## Multiple R-squared:  0.5514, Adjusted R-squared:  0.542 
## F-statistic: 58.99 on 1 and 48 DF,  p-value: 6.71e-10</code></pre>
<p>The first row is giving information about the y-intercept. In this case the estimate is <span class="math inline">\(-0.5694\)</span> and the standard error of the estimate is <span class="math inline">\(0.5217\)</span>. The t-statistic and associated p-value is testing the hypotheses: <span class="math inline">\(H_{0}:\,\beta_{0}=0\)</span> vs <span class="math inline">\(H_{a}:\,\beta_{0}\ne0\)</span>. This test is not usually of much interest. However because the equivalent test in the slope row testing <span class="math inline">\(\beta_{1}=0\)</span> vs <span class="math inline">\(\beta_{1}\ne0\)</span>, the p-value of the slope row is <em>very</em> interesting because it tells me if I should include the slope variable in the model. If <span class="math inline">\(\beta_{1}\)</span> could be zero, then we should drop the predictor from our model and use the simple model <span class="math inline">\(y_{i}=\beta_{0}+\epsilon_{i}\)</span> instead.</p>
<p>There are a bunch of other statistics that are returned by <code>summary()</code>. The Residual standard error is just <span class="math inline">\(\hat{\sigma}=\sqrt{MSE}\)</span> and the degrees of freedom for that error is also given. The rest are involved with the ANOVA interpretation of a linear model.</p>
<div id="anova-interpretation" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Anova Interpretation</h3>
<p>Just as in the ANOVA analysis, we really have a competition between two models. The full model <span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x+\epsilon_{i}\]</span> vs the simple model where x does not help predict <span class="math inline">\(y\)</span> <span class="math display">\[y_{i}=\beta_0+\epsilon_{i}\]</span> Notice this is effectively forcing the regression line to be flay and I could have written the model using <span class="math inline">\(\beta_{0}=\mu\)</span> to try to keep our notation straight. If I were to look at the simple model I would use <span class="math inline">\(\bar{y}=\hat{\beta}_0\)</span> as the predicted value of <span class="math inline">\(y\)</span> for <em>any</em> value of <span class="math inline">\(x\)</span> and my Sum of Squared Error in the simple model will be <span class="math display">\[SSE_{simple}  =   \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
    =   \sum_{i=1}^{n}\left(y_{i}-\hat{\beta}_0\right)^{2}\]</span> and the appropriate Mean Squared Error is</p>
<p><span class="math display">\[MSE_{simple}=\frac{1}{n-1}\sum\left(y_{i}-\hat{\beta}_0\right)^{2}\]</span></p>
<p>We can go through the same sort of calculations for the full complex model and get <span class="math display">\[SSE_{complex} =   \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}
    =   \sum_{i=1}^{n}\left(y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\right)\right)^{2}\]</span> Notice that <span class="math inline">\(\hat{\beta}_0\)</span> term is in both models, but will not be numerically the same. Next we have <span class="math display">\[MSE_{complex}=\frac{1}{n-2}\sum_{i=1}^{n}\left(y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\right)\right)^{2}\]</span> Just as in the AVOVA analysis, if we often like to look at the difference between <span class="math display">\[SSE_{simple}-SSE_{comples}=SSE_{diff}\]</span> and think of this quantity as the amount of variability that is explained by adding the slope parameter to the model. Just as in the AVOVA case we’ll calculate <span class="math display">\[MSE_{diff}=SSE_{diff}/df_{diff}\]</span> where <span class="math inline">\(df_{diff}\)</span> is the number of parameters that we added to the simple model to create the complex one. In the simple linear regression case, <span class="math inline">\(df_{diff}=1\)</span>.</p>
<p>Just as in the ANOVA case, we will calculate an f-statistic to test the null hypothesis that the simple model suffices vs the alternative that the complex model is necessary. The calculation is <span class="math display">\[f=\frac{MSE_{diff}}{MSE_{complex}}\]</span> and the associated p-value is <span class="math inline">\(P\left(F_{1,n-2}&gt;f\right)\)</span>. Notice that this test is exactly testing if <span class="math inline">\(\beta_{1}=0\)</span> and therefore the p-value for the F-test and the t-test for <span class="math inline">\(\beta_{1}\)</span> are the same. It can easily be shown that <span class="math inline">\(t_{1}^{2}=f\)</span>.</p>
<p>The Analysis of Variance table looks the same as what we have seen, but now we recognize that the rows actually represent the complex and simple models and the difference between them.</p>
<table style="width:100%;">
<colgroup>
<col width="9%" />
<col width="6%" />
<col width="12%" />
<col width="26%" />
<col width="29%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">df</th>
<th align="center">Sum Sq</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Difference</td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(SSE_{diff}\)</span></td>
<td align="center"><span class="math inline">\(MSE_{diff} = SSE_{diff}/1\)</span></td>
<td align="center"><span class="math inline">\(f = \frac{MSE_{diff}}{MSE_{complex}}\)</span></td>
<td align="center"><span class="math inline">\(P(F_{1,n-2} &gt; f)\)</span></td>
</tr>
<tr class="even">
<td align="center">Complex</td>
<td align="center"><span class="math inline">\(n-2\)</span></td>
<td align="center"><span class="math inline">\(SSE_{complex}\)</span></td>
<td align="center"><span class="math inline">\(MSE{complex} = SSE_{complex}/(n-2\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Simple</td>
<td align="center"><span class="math inline">\(n-1\)</span></td>
<td align="center"><span class="math inline">\(SSE_{simple}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>As usual, the ANOVA table for the regression is available in R using the <code>anova()</code> command.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width ~<span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)
<span class="kw">anova</span>(model)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Sepal.Width
##              Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Sepal.Length  1 3.8821  3.8821  58.994 6.71e-10 ***
## Residuals    48 3.1587  0.0658                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>But we notice that R chooses not to display the row corresponding to the simple model.</p>
<p>I could consider <span class="math inline">\(SSE_{simple}\)</span> as a baseline measure of the amount of variability in the data. It is interesting to look at how much of that baseline variability has been explained by adding the additional parameter to the model. Therefore we’ll define the ratio <span class="math inline">\(R^{2}\)</span> as: <span class="math display">\[R^{2}=\frac{SSE_{diff}}{SSE_{simple}}=\frac{SSE_{simple}-SSE_{complex}}{SSE_{simple}}=r^{2}\]</span> where <span class="math inline">\(r\)</span> is Pearson’s Correlation Coefficient. <span class="math inline">\(R^{2}\)</span> has the wonderful interpretation of the percent of variability in the response variable that can be explained by the predictor variable <span class="math inline">\(x\)</span>.</p>
</div>
<div id="confidence-intervals-vs-prediction-intervals" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Confidence Intervals vs Prediction Intervals</h3>
<p>There are two different types of questions that we might ask about predicting the value for some x-value <span class="math inline">\(x_{new}\)</span>.</p>
<p>We might be interested in a confidence interval for regression line. For this question we want to know how much would we expect the sample regression line move if we were to collect a new set of data. In particular, for some value of <span class="math inline">\(x\)</span>, say <span class="math inline">\(x_{new}\)</span>, how variable would the regression line be? To answer that we have to ask what is the estimated variance of <span class="math inline">\(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\)</span>? The variance of the regression line will be a function of the variances of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> and thus the standard error looks somewhat reminiscent of the standard errors of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>. Recalling that we defined <span class="math inline">\(S_{xx}=\sum\left(x_{i}-\bar{x}\right)^{2}\)</span>, we have: <span class="math display">\[\hat{Var}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)=\hat{\sigma}^{2}\left(\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}\right)\]</span> and therefore its <span class="math inline">\(StdErr(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new})\)</span> is <span class="math display">\[StdErr\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)=\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>We can use this value to produce a confidence interval for the regression line for any value of <span class="math inline">\(x_{new}\)</span>. <span class="math display">\[Estimate  \pm t\;StdErr\left(Estimate\right)\]</span> <span class="math display">\[\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)   \pm t_{n-2}^{1-\alpha/2}\;\;\hat{\sigma}\sqrt{\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>the expected value of new observation <span class="math inline">\(\hat{E}\left(Y\,|\,X=x_{new}\right)\)</span>. This expectation is regression line but because the estimated regression line is a function of the data, then the line isn’t the exactly the same as the true regression line. To reflect that, I want to calculate a confidence interval for where the true regression line should be.</p>
<p>I might instead be interested calculating a confidence interval for <span class="math inline">\(y_{new}\)</span>, which I will call a <em>prediction</em> interval in an attempt to keep from being confused with the confidence interval of the regression line. Because we have <span class="math display">\[y_{new}=\beta_{0}+\beta_{1}x_{new}+\epsilon_{new}\]</span></p>
<p>then my prediction interval will still be centered at <span class="math inline">\(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\)</span> but the the uncertainty should be the sum of the uncertainty associated with the estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> and the additional variability associated with <span class="math inline">\(\epsilon_{new}\)</span>. In short, <span class="math display">\[\begin{aligned}
\hat{Var}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}+\epsilon\right)   
    &amp;=  \hat{Var}\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)+\hat{Var}\left(\epsilon\right) \\
      &amp;=    \hat{\sigma}^{2}\left(\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}\right)+\hat{\sigma}^{2}
      \end{aligned}\]</span></p>
<p>and the <span class="math inline">\(StdErr\left(\right)\)</span> of a new observation will be</p>
<p><span class="math display">\[StdErr\left(\hat{y}_{new}\right)=\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>So the prediction interval for a new observation will be: <span class="math display">\[\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{new}\right)\pm t_{n-2}^{1-\alpha/2}\;\;\hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{\left(x_{new}-\bar{x}\right)^{2}}{S_{xx}}}\]</span></p>
<p>To emphasize the difference between confidence regions (capturing where we believe the regression line to lay) versus prediction regions (where new data observations will lay) we note that as the sample size increases, the uncertainty as to where the regression line lays decreases, but the prediction intervals will always contain a minimum width due to the error associated with an individual observation. Below are confidence (red) and prediction (blue) regions for two different sample sizes.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-242-1.png" width="672" /></p>
<p>In general, you will not want to calculate the confidence intervals and prediction intervals by hand. Fortunately R makes it easy to calculate the intervals. The function <code>predict()</code> will calculate the point estimates along with confidence and prediction intervals. The function requires the <code>lm()</code> output along with an optional data frame (if you want to predict values not in the original data).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(setosa, <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>Sepal.Width)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Sepal Length vs Sepal Width&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-243-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#fit the regression</span>
model &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Width ~<span class="st"> </span>Sepal.Length, <span class="dt">data=</span>setosa)

<span class="co"># display the first few predictions</span>
<span class="kw">head</span>( <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>) )</code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 3.503062 3.427519 3.578604
## 2 3.343356 3.267122 3.419590
## 3 3.183650 3.086634 3.280666
## 4 3.103798 2.991890 3.215705
## 5 3.423209 3.350256 3.496162
## 6 3.742620 3.632603 3.852637</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict at x = 5.0</span>
<span class="kw">predict</span>(model, 
        <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>,                  <span class="co"># prediction Interval</span>
        <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">Sepal.Length =</span> <span class="fl">5.0</span>)) <span class="co"># at x=5</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 3.423209 2.902294 3.944123</code></pre>
<p>We can create a nice graph of the regression line and associated confidence and prediction regions using the following code in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ask for the confidence and prediction intervals</span>
conf.region &lt;-<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>)
pred.region &lt;-<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;prediction&#39;</span>)

<span class="co"># add them to my original data frame</span>
setosa &lt;-<span class="st"> </span>setosa %&gt;%
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">fit =</span> <span class="kw">fitted</span>(model),
          <span class="dt">conf.lwr =</span> conf.region[,<span class="dv">2</span>],
          <span class="dt">conf.upr =</span> conf.region[,<span class="dv">3</span>],
          <span class="dt">pred.lwr =</span> pred.region[,<span class="dv">2</span>],
          <span class="dt">pred.upr =</span> pred.region[,<span class="dv">3</span>])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make a nice plot</span>
<span class="kw">ggplot</span>(setosa) +
<span class="st">  </span><span class="kw">geom_point</span>(  <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>Sepal.Width) ) +
<span class="st">  </span><span class="kw">geom_line</span>(   <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">y=</span>fit), <span class="dt">col=</span><span class="st">&#39;red&#39;</span> ) +
<span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">ymin=</span>conf.lwr, <span class="dt">ymax=</span>conf.upr), <span class="dt">fill=</span><span class="st">&#39;red&#39;</span>,  <span class="dt">alpha=</span>.<span class="dv">4</span>) +
<span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>(<span class="dt">x=</span>Sepal.Length, <span class="dt">ymin=</span>pred.lwr, <span class="dt">ymax=</span>pred.upr), <span class="dt">fill=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-246-1.png" width="672" /></p>
<p>It is worth noting that these confidence intervals are all point-wise confidence intervals. If I want to calculate confidence or prediction intervals for a large number of <span class="math inline">\(x_{new}\)</span> values, then I have to deal with the multiple comparisons issue. Fortunately this is easy to do in the simple linear regression case. Instead of using the <span class="math inline">\(t_{n-2}^{1-\alpha/2}\)</span> quantile in the interval formulas, we should use <span class="math inline">\(W=\sqrt{2*F_{1-\alpha,\,2,\,n-2}}\)</span>. Many books ignore this issue as does the <code>predict()</code> function in R.</p>
</div>
</div>
<div id="extrapolation" class="section level2">
<h2><span class="header-section-number">10.3</span> Extrapolation</h2>
<p>The data observed will inform a researcher about the relationship between the x and y variables, but only in the range for which you have data! Below are the winning times of the men’s 1500 meter Olympic race.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(men1500m, <span class="dt">package=</span><span class="st">&#39;HSAUR2&#39;</span>)
small &lt;-<span class="st"> </span>men1500m %&gt;%<span class="st"> </span><span class="kw">filter</span>( year !=<span class="st"> </span><span class="dv">1896</span> )  <span class="co"># Remove the 1896 Olympics</span>

<span class="co"># fit the model and get the prediction interval</span>
model &lt;-<span class="st"> </span><span class="kw">lm</span>( time ~<span class="st"> </span>year, <span class="dt">data=</span>small )
small &lt;-<span class="st"> </span><span class="kw">cbind</span>(small, <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;prediction&#39;</span>) )

<span class="kw">ggplot</span>(small, <span class="kw">aes</span>(<span class="dt">x=</span>year, <span class="dt">y=</span>time, <span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_line</span>( <span class="kw">aes</span>(<span class="dt">y=</span>fit), <span class="dt">col=</span><span class="st">&#39;red&#39;</span> ) +
<span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="dt">fill=</span><span class="st">&#39;light blue&#39;</span>,  <span class="dt">alpha=</span>.<span class="dv">4</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>( <span class="dt">x=</span><span class="st">&#39;Year&#39;</span>, <span class="dt">y=</span><span class="st">&#39;Time (s)&#39;</span>, <span class="dt">title=</span><span class="st">&#39;Winning times of Mens 1500 m&#39;</span> ) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-247-1.png" width="672" /></p>
<p>If we are interested in predicting the results of the 2008 and 2012 Olympic race, what would we predict?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(model, 
        <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">year=</span><span class="kw">c</span>(<span class="dv">2008</span>, <span class="dv">2012</span>)), 
        <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 208.1293 199.3971 216.8614
## 2 206.8451 198.0450 215.6453</code></pre>
<p>We can compare the predicted intervals with the time actually recorded by the winner of the men’s 1500m. In Beijing 2008, Rashid Ramzi from Brunei won the event in 212.94 seconds and in London 2012 Taoufik Makhloufi from Algeria won in 214.08 seconds. Both times are within the corresponding prediction intervals, but clearly the linear relationship must eventually change and therefore our regression could not possibly predict the winning time of the 3112 race.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(model, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">year=</span><span class="kw">c</span>(<span class="dv">3112</span>)), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<pre><code>##         fit       lwr       upr
## 1 -146.2973 -206.7705 -85.82402</code></pre>
</div>
<div id="checking-model-assumptions" class="section level2">
<h2><span class="header-section-number">10.4</span> Checking Model Assumptions</h2>
<p>As in the anova analysis, we want to be able to check the model assumptions. To do this, we will examine the residuals <span class="math inline">\(e_{i}=y_{i}-\hat{y}_{i}\)</span> for normality using a QQ-plot as we did in ANOVA. To address the constant variance and linearity assumptions we will look at scatterplots of the residuals vs the fitted values <span class="math inline">\(\hat{y}_{i}\)</span>. For the regression to be valid, we want the scatterplot to show no discernible trend. There are two patterns that commonly show up that indicate a violation of the regression assumptions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2233</span>);
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>));
n &lt;-<span class="st"> </span><span class="dv">20</span>;
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">length=</span>n);
data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">Fitted=</span><span class="kw">c</span>(x,x,x),
  <span class="dt">Residual=</span><span class="kw">c</span>(<span class="kw">rnorm</span>(n,<span class="dv">0</span>,.<span class="dv">25</span>), <span class="kw">rnorm</span>(n,(<span class="dv">2</span>*x<span class="dv">-1</span>)^<span class="dv">2</span><span class="fl">-.375</span>, .<span class="dv">2</span>), <span class="kw">rnorm</span>(n,<span class="dv">0</span>,x*.<span class="dv">45</span>)), 
  <span class="dt">Type=</span><span class="kw">factor</span>(<span class="kw">rep</span>(<span class="dv">1</span>:<span class="dv">3</span>, <span class="dt">each=</span>n), <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&#39;No Trend&#39;</span>, <span class="st">&#39;Non-Linear&#39;</span>, <span class="st">&#39;Non-Constant Variance&#39;</span>) ));
for(i in <span class="dv">1</span>:<span class="dv">3</span>){
  index &lt;-<span class="st"> </span><span class="dv">1</span>:n +<span class="st"> </span>n*(i<span class="dv">-1</span>);
  <span class="kw">plot</span>(data$Fitted[index], data$Residual[index], 
       <span class="dt">xlab=</span><span class="st">&#39;Fitted&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Residual&#39;</span>, <span class="dt">main=</span>data$Type[index[<span class="dv">1</span>]] );
  <span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">2</span>);
}</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-250-1.png" width="672" /></p>
<p>To illustrate this, we’ll consider the cherry tree dataset that comes with R. The goal will be predicting the volume of lumber produced by a cherry tree of a given diameter. The data are given in a dataset pre-loaded in R called <code>trees</code>.</p>
<p>Step one: Graph the data. The first step in a regression analysis is to graph the data and think about if a linear relationship makes sense.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(trees)  <span class="co"># 3 columns Girth, Height, Volume</span></code></pre></div>
<pre><code>##   Girth Height Volume
## 1   8.3     70   10.3
## 2   8.6     65   10.3
## 3   8.8     63   10.2
## 4  10.5     72   16.4
## 5  10.7     81   18.8
## 6  10.8     83   19.7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(trees, <span class="kw">aes</span>(<span class="dt">x=</span>Girth, <span class="dt">y=</span>Volume)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Volume vs Girth&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-251-1.png" width="672" /></p>
<p>Initially, it looks like a line is a pretty good description of this relationship.</p>
<p>Step two: Fit a regression and examine the diagnostic plots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>( Volume ~<span class="st"> </span>Girth, <span class="dt">data=</span>trees )
<span class="kw">autoplot</span>(model, <span class="dt">which=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-252-1.png" width="672" /></p>
<p>The normality assumption isn’t too bad, but there is a strong trend in the residual plot. The curvature we see in the residual group is present in the original scatterplot, but it is more obvious. At this point I would think about a slightly more complicated model, e.g. should we include height in the model or perhaps <code>Girth^2</code>? The implications of both of these possibilities will be explored in STA 571 but for now we’ll just continue using the model we have.</p>
<p>Step three: Plot the data and the regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees &lt;-<span class="st"> </span><span class="kw">cbind</span>( trees, <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>) )
<span class="kw">head</span>(trees)  <span class="co"># now we have the fit, lwr, upr columns</span></code></pre></div>
<pre><code>##   Girth Height Volume       fit       lwr       upr
## 1   8.3     70   10.3  5.103149  2.152294  8.054004
## 2   8.6     65   10.3  6.622906  3.799685  9.446127
## 3   8.8     63   10.2  7.636077  4.896577 10.375578
## 4  10.5     72   16.4 16.248033 14.156839 18.339228
## 5  10.7     81   18.8 17.261205 15.235884 19.286525
## 6  10.8     83   19.7 17.767790 15.774297 19.761284</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(trees, <span class="kw">aes</span>(<span class="dt">x=</span>Girth)) +
<span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>( <span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr), <span class="dt">alpha=</span>.<span class="dv">4</span>, <span class="dt">fill=</span><span class="st">&#39;pink&#39;</span> ) +
<span class="st">  </span><span class="kw">geom_line</span>( <span class="kw">aes</span>(<span class="dt">y=</span>fit), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>Volume)) </code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-253-1.png" width="672" /></p>
<p>In this graph we see that we underestimate the volume for small girths, overestimate for medium values, and underestimate for large girths. So we see the same pattern of the residuals in this graph as we saw in the residual graph. While the model we’ve selected isn’t as good as it could be, this isn’t horribly bad and might suffice for a first pass</p>
<blockquote>
<p>“All models are wrong, but some are useful.” George Box.</p>
</blockquote>
<p>Step four: Evaluate the model coefficients.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Volume ~ Girth, data = trees)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.065 -3.107  0.152  3.495  9.587 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***
## Girth         5.0659     0.2474   20.48  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.252 on 29 degrees of freedom
## Multiple R-squared:  0.9353, Adjusted R-squared:  0.9331 
## F-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(model)</code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) -43.825953 -30.060965
## Girth         4.559914   5.571799</code></pre>
<p>From the summary output, we can see several things:</p>
<ol style="list-style-type: decimal">
<li><p>The intercept term <span class="math inline">\(\hat{\beta}_{0}\)</span> is significantly different than zero. While we should expect that a tree with zero girth should have zero volume, our model predicts a volume of -36.9, which is obviously ridiculous. I’m not too worried about this because we have no data from trees that small and the intercept is quite the extrapolation from the range of Girth values we actually have. This is primarily being driven by the real relationship having curvature and our model has no curvature in it. So long as we don’t use this model to predict values too far away from our data points, I’m happy.</p></li>
<li><p>The slope is statistically significantly positive. We see an estimate an increase of 5 units of Volume for every 1 unit increase in Girth.</p></li>
<li><p>The estimate <span class="math inline">\(\hat{\sigma}\)</span> is given by the residual standard error and is 4.252 and that is interpreted as the typical distance away from the regression line.</p></li>
<li><p>The R-sq value gives the amount of variability in the data that is explained by the regression line as <span class="math inline">\(93.5\%\)</span>. So the variable Girth explains a huge amount of the variability in volume of lumber a tree produces.</p></li>
<li><p>Finally, the F-test is comparing the complex vs the simple model, which in this case, reduces to just testing if the slope term, <span class="math inline">\(\beta_{1}\)</span>, could be zero. In simple regression, the F-statistic is the square of the t-statistic for testing the slope. That is, F-statistic = <span class="math inline">\(419.4 = 20.48^{2}\)</span>. The p-values are the same for the two tests because they are testing exactly the same hypothesis.</p></li>
</ol>
</div>
<div id="common-problems" class="section level2">
<h2><span class="header-section-number">10.5</span> Common Problems</h2>
<div id="influential-points" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Influential Points</h3>
<p>Sometimes a dataset will contain one observation that has a large effect on the outcome of the model. Consider the following datasets where the red denotes a highly influential point and the red line is the regression line including the point.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-255-1.png" width="672" /></p>
<p>The question of what to do with influential points is not easy to answer. Sometimes these are data points that are a result of lab technician error and should be removed. Sometimes they are the result of an important process that is not well understood by the researcher. It is up to the scientist to figure out which is the case and take appropriate action.</p>
<p>One solution is to run the analysis both with and without the influential point and see how much it affects your inferences.</p>
</div>
<div id="transformations" class="section level3">
<h3><span class="header-section-number">10.5.2</span> Transformations</h3>
<p>When the normality or constant variance assumption is violated, sometimes it is possible to transform the data to make it satisfy the assumption. Often times count data is analyzed as log(count) and weights are analyzed after taking a square root or cube root transform.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-256-1.png" width="672" /></p>
<p>We have the option of either transforming the x-variable or transforming the y-variable or possibly both. One thing to keep in mind, however, is that transforming the x-variable only effects the linearity of the relationship. Transforming the y-variable effects both the linearity and the variance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(-<span class="dv">838</span>)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))
n &lt;-<span class="st"> </span><span class="dv">40</span>
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>, <span class="dt">length=</span>n);
y &lt;-<span class="st"> </span><span class="dv">2</span> +<span class="st"> </span><span class="dv">30</span>*<span class="kw">exp</span>((<span class="dv">30</span>-x)/<span class="dv">10</span>) +<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span><span class="dv">20</span>)
y &lt;-<span class="st"> </span><span class="kw">abs</span>(y)
<span class="kw">plot</span>(x,y); <span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(y~x)));
<span class="kw">plot</span>(x, <span class="kw">log</span>(y)); <span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(<span class="kw">I</span>(<span class="kw">log</span>(y))~x)));
<span class="kw">plot</span>(x^(<span class="dv">1</span>/<span class="dv">3</span>), y); <span class="kw">abline</span>(<span class="kw">coef</span>(<span class="kw">lm</span>(y~<span class="kw">I</span>(x^(<span class="dv">1</span>/<span class="dv">3</span>)))));</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-257-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mydata &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)</code></pre></div>
<p>Unfortunately it is not always obvious what transformation is most appropriate. The Box-Cox family of transformations for the y-variable is <span class="math display">\[ f(y\,|\,\lambda) =   \begin{cases}
    y^{\lambda} &amp; \;\;\textrm{if}\,\,\lambda\ne0\\
    \log y &amp; \;\;\textrm{if}\,\,\lambda=0
  \end{cases}\]</span> which includes squaring (<span class="math inline">\(\lambda=2\)</span>), square root (<span class="math inline">\(\lambda=1/2\)</span>) and as <span class="math inline">\(\lambda \to 0\)</span> the transformation converges to <span class="math inline">\(\log y\)</span>. (To do this correctly we should define the transformation in a more complicated fashion, but that level of detail is unnecessary here.) The transformation is selected by looking at the profile log-likelihood value of different values of <span class="math inline">\(\lambda\)</span> and we want to use the <span class="math inline">\(\lambda\)</span> that maximizes the log-likelihood.</p>
<p>Of course, we also want to use a transformation that isn’t completely obscure and is commonly used in the scientific field, so square roots, reciprocals, and logs are preferred.</p>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-258-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(mydata)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    40 obs. of  2 variables:
##  $ x: num  0 0.769 1.538 2.308 3.077 ...
##  $ y: num  2 3.08 2.92 4.17 5.44 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MASS::<span class="kw">boxcox</span>(y~x, <span class="dt">data=</span>mydata, <span class="dt">plotit=</span><span class="ot">TRUE</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-259-1.png" width="672" /></p>
<p>Here we see the resulting confidence interval for <span class="math inline">\(\lambda\)</span> contains 0, so a <span class="math inline">\(\log\)</span> transformation would be most appropriate.</p>
<p>Unfortunately there isn’t a matching procedure for deciding how to transform the <span class="math inline">\(x\)</span> covariate. Usually we spend a great deal of time trying different transformations and see how they affect the scatterplot and using transformations that are common in whatever field the researcher is working in.</p>
<p>In general, deciding on a transformation to use is often a trade-off between statistical pragmatism and interpretability. In cases that a transformation is not possible, or the interpretation is difficult, it is necessary to build more complicated models that are hopefully interpretable. We will explore these issues in great length in STA 571.</p>
</div>
</div>
<div id="exercises-9" class="section level2">
<h2><span class="header-section-number">10.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Use the following data below to answer the questions below</p>
<table>
<tbody>
<tr class="odd">
<td align="center"><strong>x</strong></td>
<td align="center">3</td>
<td align="center">8</td>
<td align="center">10</td>
<td align="center">18</td>
<td align="center">23</td>
<td align="center">28</td>
</tr>
<tr class="even">
<td align="center"><strong>y</strong></td>
<td align="center">14</td>
<td align="center">28</td>
<td align="center">43</td>
<td align="center">62</td>
<td align="center">79</td>
<td align="center">86</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p>Plot the data in a scatter plot. <em>The following code might be useful:</em></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># read in the data</span>
p1.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">x =</span> <span class="kw">c</span>( <span class="dv">3</span>,  <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">18</span>, <span class="dv">23</span>, <span class="dv">28</span>),
  <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">14</span>, <span class="dv">28</span>, <span class="dv">43</span>, <span class="dv">62</span>, <span class="dv">79</span>, <span class="dv">86</span>)  )

<span class="co"># make a nice graph</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(p1.data, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div></li>
<li><p>We will first calculate the regression coefficients and their estimated standard deviations by hand (mostly).</p>
<ol style="list-style-type: lower-roman">
<li>Use R to confirm that that the following summary statistics are correct:</li>
</ol>
<table>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\bar{x}=15\)</span></td>
<td align="center"><span class="math inline">\(s_x=9.59\)</span></td>
<td align="center"><span class="math inline">\(S_{xx}=460\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\bar{y}=52\)</span></td>
<td align="center"><span class="math inline">\(s_y=28.59\)</span></td>
<td align="center"><span class="math inline">\(r = 0.9898\)</span></td>
</tr>
</tbody>
</table>
<ol start="2" style="list-style-type: lower-roman">
<li><p>Using the above statistics, by hand calculate the estimates <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>.</p></li>
<li><p>For each data point, by hand calculate the predicted value <span class="math inline">\(\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\)</span>.</p></li>
<li><p>For each data point, by hand calculate the estimated error term <span class="math inline">\(\hat{\epsilon}_{i}=y_{i}-\hat{y}_{i}\)</span>.</p></li>
<li><p>Calculate the MSE for the complex model. Using the MSE, what is <span class="math inline">\(\hat{\sigma}\)</span>?</p></li>
<li><p>By hand, calculate the estimated standard deviation (which is often called the standard error) of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>.</p></li>
</ol></li>
<li><p>Use the R function <code>lm()</code> to fit a regression to these data.</p>
<ol style="list-style-type: lower-roman">
<li><p>Using the <code>predict()</code> function, confirm your hand calculation of the <span class="math inline">\(\hat{y}_{i}\)</span> values.</p></li>
<li><p>Using the <code>resid()</code> function, confirm your hand calculation of the <span class="math inline">\(\hat{\epsilon}_{i}\)</span> terms.</p></li>
<li><p>Using the <code>summary()</code> function, confirm your hand calculations of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> and their standard errors.</p></li>
</ol></li>
<li><p>Again using R’s built in functions, give a 95% confidence interval for <span class="math inline">\(\beta_{1}\)</span>.</p></li>
<li><p>Using the appropriate R output, test the hypothesis <span class="math inline">\(H_{0}:\;\beta_{1}=0\)</span> versus the alternative <span class="math inline">\(H_{a}:\;\beta_{1} \ne 0\)</span>.</p></li>
<li><p>Give the R^{2} value for this regression.</p></li>
<li><p>What is the typical distance to the regression line?</p></li>
<li><p>Create a nice graph of the regression line and the confidence interval for the true relationship using the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make a nice graph</span>
<span class="kw">ggplot</span>(p1.data, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>)</code></pre></div>
<p>Often I want to create the confidence region myself (perhaps to use a prediction interval instead of a confidence interval), and we could use the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
model &lt;-<span class="st"> </span><span class="kw">lm</span>( y ~<span class="st"> </span>x, <span class="dt">data=</span>p1.data )

p1.data &lt;-<span class="st"> </span>p1.data %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">yhat =</span> <span class="kw">predict</span>(model),
          <span class="dt">lwr  =</span> <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>)[,<span class="dv">2</span>],
          <span class="dt">upr  =</span> <span class="kw">predict</span>(model, <span class="dt">interval=</span><span class="st">&#39;confidence&#39;</span>)[,<span class="dv">3</span>]  )

<span class="co"># make a nice graph</span>
<span class="kw">ggplot</span>(p1.data, <span class="kw">aes</span>(<span class="dt">x=</span>x)) +
<span class="st">  </span><span class="kw">geom_ribbon</span>( <span class="kw">aes</span>(<span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr), <span class="dt">fill=</span><span class="st">&#39;pink&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">2</span> ) +
<span class="st">  </span><span class="kw">geom_line</span>(   <span class="kw">aes</span>(   <span class="dt">y=</span>yhat), <span class="dt">color=</span><span class="st">&#39;green&#39;</span> ) +
<span class="st">  </span><span class="kw">geom_point</span>(  <span class="kw">aes</span>(   <span class="dt">y=</span>y   ), <span class="dt">color=</span><span class="st">&#39;black&#39;</span> )</code></pre></div></li>
</ol></li>
<li><p>Olympic track and field records are broken practically every Olympics. The following is output comparing the gold medal winning performance in the men’s long jump (in inches) versus the years 00 to 84. (In this data set, the year 00 represents 1900, and 84 represents 1984. This is a pre Y2K dataset.) There were <span class="math inline">\(n=19\)</span> Olympic games in that period.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fill in the blanks in the following summary and anova tables:</p>
<p>Summary:</p>
<table>
<thead>
<tr class="header">
<th align="center">Coefficients</th>
<th align="center">Estimate</th>
<th align="center">Std Error</th>
<th align="center">t-value</th>
<th align="center">$Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">(Intercept)</td>
<td align="center">283.45</td>
<td align="center">4.28</td>
<td align="center"></td>
<td align="center">&lt; 2e-16</td>
</tr>
<tr class="even">
<td align="center">Year</td>
<td align="center">0.613</td>
<td align="center">0.0841</td>
<td align="center">7.289</td>
<td align="center">1.27e-06</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="60%" />
<col width="39%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Residual Standard Error = <span class="math inline">\(\;\;\;\;\;\;\;\;\;\;\)</span></td>
<td align="center">R-sq = <span class="math inline">\(\;\;\;\;\;\;\;\;\;\;\)</span></td>
</tr>
</tbody>
</table>
<p>Analysis of Variance:</p>
<table>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">df</th>
<th align="center">Sum Sq</th>
<th align="center">Mean Sq</th>
<th align="center">F-value</th>
<th align="center">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Year</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Residuals</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">95.19</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center">18</td>
<td align="center">6673.2</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table></li>
</ol></li>
<li><p>Ott &amp; Longnecker 11.45&amp;47 - In the preliminary studies of a new drug, a pharmaceutical firm needs to obtain information on the relationship between the dose level and potency of the drug. In order to obtain this information, a total of 18 test tubes are inoculated with a virus culture and incubated for an appropriate period of time. Three test tubes are randomly assigned to each of 6 different dose levels. The 18 test tubes are then injected with the randomly assigned dose level of the drug. the measured response is the protective strength of the drug against the virus culture. Due to a problem with a few of the test tubes, only 2 responses were obtained for dose levels 4,8, and 16. The data are:</p>
<table>
<tbody>
<tr class="odd">
<td align="center">Dose</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">8</td>
<td align="center">8</td>
<td align="center">16</td>
<td align="center">16</td>
<td align="center">16</td>
<td align="center">32</td>
<td align="center">32</td>
<td align="center">64</td>
<td align="center">64</td>
<td align="center">64</td>
</tr>
<tr class="even">
<td align="center">Response</td>
<td align="center">5</td>
<td align="center">7</td>
<td align="center">3</td>
<td align="center">10</td>
<td align="center">14</td>
<td align="center">15</td>
<td align="center">17</td>
<td align="center">20</td>
<td align="center">21</td>
<td align="center">19</td>
<td align="center">23</td>
<td align="center">29</td>
<td align="center">28</td>
<td align="center">31</td>
<td align="center">30</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>We will first fit a regression model to the raw data.
<ol style="list-style-type: lower-roman">
<li>Plot the data and comment on the relationship between the covariate and response.</li>
<li>Fit a linear regression model to these data using the lm() function.</li>
<li>Examine the plot of the residuals vs fitted values. Does there appear to be a problem? Explain.</li>
</ol></li>
<li>Often in drug evaluations, a logarithmic transformation of the dose level will yield a linear relationship between the response variable and the independent variable. Let <span class="math inline">\(x_{i}=\log\left(dose_{i}\right)\)</span> (where log is the natural log). Notice that because the constant variance assumption seems to be met, I don’t wish to transform <span class="math inline">\(y\)</span>.
<ol style="list-style-type: lower-roman">
<li>Plot the response of the drug vs the natural log of the dose levels. Does it appear that a linear model is appropriate?</li>
<li>Fit the linear regression model to these data.</li>
<li>From a plot of the residuals vs the fitted values, does the linear model seem appropriate?</li>
<li>Examine the QQplot of the residuals vs the theoretical normal quantiles. Does the normality assumption appear to be violated? Also perform a Shapiro-Wilks test on the residuals to test of a statistically significant difference from normality. Comment on these results.</li>
<li>What is change in the response variable for every one unit change in log(dose)?</li>
<li>Give a <span class="math inline">\(95\%\)</span> confidence interval for the y-intercept and slope parameters. Is the log(dose) level a statistically significant predictor of the response?</li>
</ol></li>
</ol></li>
</ol>

</div>
<div id="contingency-tables" class="section level2">
<h2><span class="header-section-number">10.7</span> Contingency Tables</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)</code></pre></div>
<p>We are often interested in experiments and studies where the response variable is categorical and so is the explanatory.</p>
<ul>
<li><p>Treat plots with either Type A or Type B insecticides and after 2 weeks observe if the plots are infested or not infested with some insect.</p></li>
<li><p>Using survey data, we would like to investigate if there is a relationship between Gender and Political Party affiliation. (Are women more likely to be Democrates?)</p></li>
<li><p>Are children that are born second or third (or more!) more likely to be gay than the firstborn child?</p></li>
</ul>
<p>We will be interested in testing the null hypothesis of “No association” between the explanatory and response variable.</p>
<p>We will have two questions:</p>
<ol style="list-style-type: decimal">
<li><p>What statistic could be calculated from the observed data to measure how far the observed data is from the null hypothesis?</p></li>
<li><p>Given the statistic in part 1, how should it vary from sample to sample assuming the null hypothesis (no difference in treatments) is true?</p></li>
</ol>
</div>
<div id="expected-counts" class="section level2">
<h2><span class="header-section-number">10.8</span> Expected Counts</h2>
<p>We will develope our ideas using a subsample of data from surveys of undergraduate students in an Introductory statistics course. We will utilize <span class="math inline">\(40\)</span> males and <span class="math inline">\(40\)</span> females and consider the historical assumption that women should perform better on the verbal part of the SAT rather than the MATH part compared to their male counterparts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(StudentSurvey, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)
StudentSurvey &lt;-<span class="st"> </span>StudentSurvey %&gt;%
<span class="st">  </span><span class="kw">filter</span>( HigherSAT !=<span class="st"> &#39;&#39;</span>) %&gt;%<span class="st">    </span><span class="co"># remove a student that did not report SAT scores</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HigherSAT =</span> <span class="kw">factor</span>(HigherSAT)) %&gt;%<span class="st"> </span><span class="co"># remove the MISSING level from the above student</span>
<span class="st">  </span><span class="kw">group_by</span>(Gender) %&gt;%<span class="st">          </span><span class="co"># Only consider the first 40 males</span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>:<span class="dv">40</span>) %&gt;%<span class="st">               </span><span class="co"># and Females... as a first example</span>
<span class="st">  </span><span class="kw">select</span>(Gender, HigherSAT) </code></pre></div>
<p>In this example, exactly <span class="math inline">\(60\%\)</span> of the students had a higher score on the math portion of the SAT than on the verbal. If the null hypothesis is true, then <span class="math inline">\(60\%\)</span> of the <span class="math inline">\(40\)</span> males should have a higher Math SAT score than verbal. So under the null, we expect to see <span class="math inline">\(40 * 0.60 = 24\)</span> males and <span class="math inline">\(40*0.60=24\)</span> females to have a higher Math SAT than verbal. Similarly we would expect <span class="math inline">\(40*0.40=16\)</span> males and <span class="math inline">\(16\)</span> females to score higher on the verbal section. Below is a table that summarizes both our observed data and the expected values under the null hypotheses of no association between superior SAT category with gender.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tab &lt;-<span class="st"> </span>mosaic::<span class="kw">tally</span>( HigherSAT ~<span class="st"> </span>Gender, <span class="dt">data=</span>StudentSurvey, <span class="dt">format=</span><span class="st">&#39;count&#39;</span>)
descr::<span class="kw">CrossTable</span>( tab, <span class="dt">expected =</span> <span class="ot">TRUE</span>,
                   <span class="dt">prop.r =</span> <span class="ot">FALSE</span>, <span class="dt">prop.c=</span><span class="ot">FALSE</span>, <span class="dt">prop.t=</span><span class="ot">FALSE</span>, <span class="dt">prop.chisq =</span> <span class="ot">FALSE</span> )</code></pre></div>
<pre><code>##    Cell Contents 
## |-------------------------|
## |                       N | 
## |              Expected N | 
## |-------------------------|
## 
## ============================
##              Gender
## HigherSAT     F    M   Total
## ----------------------------
## Math         23   25      48
##              24   24        
## ----------------------------
## Verbal       17   15      32
##              16   16        
## ----------------------------
## Total        40   40      80
## ============================</code></pre>
<p>Notice that the excected cell counts can be written as <span class="math display">\[E_{ij} = \frac{ n_{i,\cdot}}{n} * n_{\cdot, j} = \frac{n_{i,\cdot} n_{\cdot,j}}{n}\]</span> where <span class="math inline">\(n_{i,\cdot}\)</span> is row total for the <span class="math inline">\(i\)</span>th row, <span class="math inline">\(n_{\cdot,j}\)</span> is the column total for the <span class="math inline">\(j\)</span>th row, and <span class="math inline">\(n\)</span> is the total number of observations in the table.</p>
<p>This is the first case where our test statistic will not be just plugging in the sample statistic into the null hypothesis. Instead we will consider a test statistic that is more flexible and will handle more general cases (say 3 or more response or treatment groups) Our statistic for assessing how far our observed data is from what we expect under the null hypothesis involves the difference between the observed and the expected for each of the cells, but again we don’t want to just sum the differences, instead will make the differences positive by squaring the differences. Second, a difference of 10 between the observed and expected cell count is very different if the number expected is 1000 than if it is 10, so we will scale the observed difference by dividing by the expected cell count.</p>
<p>We define <span class="math display">\[\begin{aligned}X^{2}  
  &amp;=    \sum_{\textrm{all ij cells}}\frac{\left(O_{ij}-E_{ij}\right)^{2}}{E_{ij}} \\
  &amp;=  \frac{(23-24)^2}{24} + \frac{(25-24)^2}{24} + \frac{(17-16)^2}{16} + \frac{(15-16)^2}{16} \\
  &amp;=  0.04167 + 0.04167 + 0.0625 + 0.0625 \\
  &amp;= 0.20834
  \end{aligned}\]</span></p>
<p>In the next section we will address if this test statistic is large enough to reject the null hypothesis.</p>
<p><em>Example</em></p>
<p>Researchers suspected that attack of a plant by one organism induce resistance to subsequent attack by a different organism. The <span class="math inline">\(47\)</span> individually potted cotton plants were randomly allocated to two groups: infestation by spider mites or no infestation. After two weeks the mites were dutifully removed by a conscientious research assistant, and both groups were inoculated with Verticillium, a fungus that causes Wilt disease.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Mites, <span class="dt">package=</span><span class="st">&quot;mosaicData&quot;</span>)
<span class="kw">str</span>(Mites)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    47 obs. of  2 variables:
##  $ treatment: Factor w/ 2 levels &quot;mites&quot;,&quot;no mites&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ outcome  : Factor w/ 2 levels &quot;no wilt&quot;,&quot;wilt&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>We will summarize the data into a contingency table that counts the number of plants in each treatment/wilt category.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Using mosaic&#39;s tally function</span>
tab &lt;-<span class="st"> </span>mosaic::<span class="kw">tally</span>(outcome ~<span class="st"> </span>treatment, <span class="dt">data=</span>Mites, <span class="co"># table of outcome by treatment</span>
              <span class="dt">format=</span><span class="st">&#39;count&#39;</span>)                  <span class="co"># give the raw counts, not percentages</span>
tab</code></pre></div>
<pre><code>##          treatment
## outcome   mites no mites
##   no wilt    15        4
##   wilt       11       17</code></pre>
<p>From this table we can see that <span class="math inline">\(28\)</span> out of the <span class="math inline">\(47\)</span> plants wilted, so the proportion that wilted was <span class="math inline">\(\frac{28}{47}=0.596\)</span>. Therefore under the null hypothesis we would expect that <span class="math inline">\(59.6\%\)</span> of the <span class="math inline">\(26\)</span> mite treated plants would have wilted, or <span class="math display">\[\left( \frac{28}{47} \right) 26 = 15.49\]</span></p>
<p>Similar calculations revel the rest of the expected cell counts.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">descr::<span class="kw">CrossTable</span>( tab, <span class="dt">expected =</span> <span class="ot">TRUE</span>,
                   <span class="dt">prop.r =</span> <span class="ot">FALSE</span>, <span class="dt">prop.c=</span><span class="ot">FALSE</span>, <span class="dt">prop.t=</span><span class="ot">FALSE</span>, <span class="dt">prop.chisq =</span> <span class="ot">FALSE</span> )</code></pre></div>
<pre><code>##    Cell Contents 
## |-------------------------|
## |                       N | 
## |              Expected N | 
## |-------------------------|
## 
## ===================================
##            treatment
## outcome    mites   no mites   Total
## -----------------------------------
## no wilt       15          4      19
##             10.5        8.5        
## -----------------------------------
## wilt          11         17      28
##             15.5       12.5        
## -----------------------------------
## Total         26         21      47
## ===================================</code></pre>
<p>Is this data indicative of mites inferring a disease resistance? More formally we are interested in testing <span class="math display">\[H_{0}:\:  \pi_{w}=\pi_{w|m}\]</span> <span class="math display">\[H_{0}:\:  \pi_{w}\ne\pi_{w|m}\]</span><br />
where the relevant parameters are <span class="math inline">\(\pi_{w}\)</span>, the probability that a plant will wilt, and <span class="math inline">\(\pi_{w|m}\)</span>, the probability that a plant will wilt given that it has been treated with mites.</p>
<p>We calculate our test statistic as <span class="math display">\[\begin{aligned}X^{2}  
  &amp;=    \sum_{\textrm{all ij cells}}\frac{\left(O_{ij}-E_{ij}\right)^{2}}{E_{ij}} \\
    &amp;=  \frac{\left(15-10.51\right)^{2}}{10.51}+\frac{\left(4-8.49\right)^{2}}{8.49}+\frac{\left(11-15.49\right)^{2}}{15.49}+\frac{\left(17-12.51\right)^{2}}{12.51}\\
    &amp;=  1.92+2.37+1.30+1.61 \\
    &amp;=  7.20
    \end{aligned}\]</span></p>
<p>If the null hypothesis is true, then this statistic should be small, and a large value of the statistic is indicative of the null hypothesis being incorrect. But how large must the statistic be before we reject the null hypothesis?</p>
<div id="hypothesis-testing" class="section level3">
<h3><span class="header-section-number">10.8.1</span> Hypothesis Testing</h3>
<p>Similarly to the two-sample t-test, we randomly shuffle the treatment assignments and recalculate the statistic many times and examine the sampling distribution of our test statistic, <span class="math inline">\(X^{2}\)</span>.</p>
<p>To do this efficiently, we’ll need a way of easily calculating this test statistic. In a traditional course I would introduce this test by the name of “Pearson’s Chi-squared test” and we can obtain the test statistic using the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># function is chisq.test() and we need to tell it not to do the Yates continuity </span>
<span class="co"># correction and just calculate the test statistic as we&#39;ve described </span>
<span class="kw">chisq.test</span>( <span class="kw">table</span>(Mites), <span class="dt">correct=</span><span class="ot">FALSE</span> )   <span class="co"># do a Chi-sq test </span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  table(Mites)
## X-squared = 7.2037, df = 1, p-value = 0.007275</code></pre>
<p>R is performing the traditional Pearson’s Chi-Squared test which assumes our sample sizes are large enough for several approximations to be good. Fortunately, we don’t care about this approximation to the p-value and will use simulation methods which will be more accurate. In order to use the <code>chisq.test()</code> function to do our calculations, we need to extract the test-statistic from the output of the function. &lt;<warning=FALSE>&gt;=</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract the X^2 test statistic from the output</span>
X.sq &lt;-<span class="st"> </span><span class="kw">chisq.test</span>( <span class="kw">table</span>(Mites), <span class="dt">correct=</span><span class="ot">FALSE</span> )$statistic <span class="co"># grab only the test statistic</span>
X.sq</code></pre></div>
<pre><code>## X-squared 
##  7.203748</code></pre>
<p>Next we wish to repeat our shuffling trick of the treatment labels to calculate the sampling distribution of <span class="math inline">\(X^{2*}\)</span>, which is the distribution of <span class="math inline">\(X^{2}\)</span> when the null hypothesis of no difference between treatments is true.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Mites.star &lt;-<span class="st"> </span>Mites %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">treatment =</span> mosaic::<span class="kw">shuffle</span>(treatment))
<span class="kw">table</span>(Mites.star)</code></pre></div>
<pre><code>##           outcome
## treatment  no wilt wilt
##   mites         11   15
##   no mites       8   13</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( <span class="kw">table</span>(Mites.star), <span class="dt">correct=</span><span class="ot">FALSE</span> )$statistic <span class="co"># grab only the test statistic</span></code></pre></div>
<pre><code>##  X-squared 
## 0.08559517</code></pre>
<p>We see that this code is creating a data frame with a single column called <code>X.squared</code> and next we simulate a large number of times and display the sampling distribution of <span class="math inline">\(X^{2*}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SamplingDist &lt;-<span class="st"> </span>mosaic::<span class="kw">do</span>(<span class="dv">10000</span>)*{
  Mites.star &lt;-<span class="st"> </span>Mites %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">treatment =</span> mosaic::<span class="kw">shuffle</span>(treatment))
  <span class="kw">chisq.test</span>( <span class="kw">table</span>(Mites.star), <span class="dt">correct=</span><span class="ot">FALSE</span> )$statistic 
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>( SamplingDist, <span class="kw">aes</span>(<span class="dt">x=</span>X.squared)) +<span class="st"> </span><span class="kw">geom_histogram</span>()</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-273-1.png" width="672" /></p>
<p>At first glance this seems wrong because it is not a nice looking distribution. However there are only a small number of ways to allocate the treatments labels to the two possible outcomes. Second, for the test statistic we have chosen only the right hand side of the distribution (large values of <span class="math inline">\(X^{*}\)</span>) would be evidence against the null hypothesis, so we only look at <span class="math inline">\(X^{2*}&gt;7.20\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p.value &lt;-<span class="st"> </span>SamplingDist %&gt;%<span class="st"> </span><span class="kw">summarize</span>( <span class="dt">p.value =</span> <span class="kw">mean</span>( X.squared &gt;=<span class="st"> </span>X.sq ) )
p.value</code></pre></div>
<pre><code>##   p.value
## 1  0.0161</code></pre>
<p>We see that the p-value is 0.0161 and conclude that there is strong evidence to reject the null hypothesis that the mite treatment does not affect the probability of wilting. That is to say, the probability of observing data as extreme as ours is unlikely to occur by random chance when the null hypothesis is true.</p>
<p>As usual, it is pretty annoying to have to program the permutation test ourselves. Fortunately the <code>chisq.test()</code> function allows us to option to tell it to do a permutation based test. There is an option <code>simulate.p.value</code> which reproduces the simulation test we just performed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( <span class="kw">table</span>(Mites), <span class="dt">simulate.p.value=</span><span class="ot">TRUE</span>, <span class="dt">B=</span><span class="dv">10000</span> )</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 10000
##  replicates)
## 
## data:  table(Mites)
## X-squared = 7.2037, df = NA, p-value = 0.0153</code></pre>
<p>Before we had our excellent computers, we would have to compare the observed <span class="math inline">\(X^{2}\)</span> test statistic to some distribution to determine if it is large enough to be evidence against the null. It can be shown that if the null hypothesis is correct then <span class="math inline">\(X^{2}\stackrel{\cdot}{\sim}\chi_{1}^{2}\)</span> where this is the Chi-squared distribution with 1 degree of freedom. This is the distribution that the <code>chisq.test()</code> compares against if we don’t tell it to do a permutation based test. Furthermore, even if the null hypothesis is true the test statistic is only approximately normal but that approximation gets better and better as the total sample size increases.</p>
<p>The reason that we compare against a Chi-squared distribution with 1 degree of freedom is because when we shuffle the group labels, we still have the same number of wilted/non-wilted plants as well as the same number of mite/no-mite treated plants. So the row and column totals are identical in all the permuted tables. So once the number of observations in the <span class="math inline">\((1,1)\)</span> cell is decided, the other three cells are also indirectely determined as well due to the row/column totals being constant regardless of permutation. In the general case with <span class="math inline">\(R\)</span> rows and <span class="math inline">\(C\)</span> columns, the number of cells that are not set due to the row/column totals, is <span class="math inline">\((R-1)(C-1)\)</span>.</p>
<p>The asymptotic approximation is usually acceptable if the observed count in each cell is greater than 5. Even then, a slightly better approximation can be obtained by using the Yates’ continuity correction. Typically I will perform the analysis both ways and confirm we get the same inference. If the two methods disagree, I’d trust the permutation method.</p>
<p><em>Example</em>:</p>
<p>In a study to investigate possible treatments for human infertility, researchers (Harrison, R. F., Blades, M., De Louvois, J., &amp; Hurley, R. (1975). Doxycycline treatment and human infertility. The Lancet, 305(7907), 605-607.) performed a double-blind study and randomly divided 58 patients into two groups. The treatment group (<span class="math inline">\(n_{t}=30\)</span>) received 100 mg per day of Doxycycline and the placebo group (<span class="math inline">\(n_{p}=28\)</span>) received a placebo but were unaware that it was a placebo. Within 5 months, the treatment group had 5 pregnancies, while the placebo group had 4. Just looking at the observed vs expected there doesn’t seem to be much difference between the treatments. In fact, due to the discrete nature of the data (i.e. integer values) we can’t imagine data that any closer to the expected value that what we observed. The p-value here ought to be 1! To confirm this we do a similar test as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Conceived &lt;-<span class="st">  </span><span class="kw">data.frame</span>(
  <span class="dt">Treatment=</span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;Doxycyline&#39;</span>,<span class="dv">30</span>), <span class="kw">rep</span>(<span class="st">&#39;Placebo&#39;</span>,<span class="dv">28</span>)),
  <span class="dt">Outcome=</span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;Conceived&#39;</span>,<span class="dv">5</span>), <span class="kw">rep</span>(<span class="st">&#39;Not Conceived&#39;</span>,<span class="dv">25</span>),
            <span class="kw">rep</span>(<span class="st">&#39;Conceived&#39;</span>,<span class="dv">4</span>), <span class="kw">rep</span>(<span class="st">&#39;Not Conceived&#39;</span>,<span class="dv">24</span>)))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use the CrossTable function to generate the Expected Cell values</span>
descr::<span class="kw">CrossTable</span>(<span class="kw">table</span>(Conceived), <span class="dt">expected=</span><span class="ot">TRUE</span>, 
                  <span class="dt">prop.r=</span><span class="ot">FALSE</span>, <span class="dt">prop.c=</span><span class="ot">FALSE</span>, <span class="dt">prop.t=</span><span class="ot">FALSE</span>, <span class="dt">prop.chisq=</span><span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##    Cell Contents 
## |-------------------------|
## |                       N | 
## |              Expected N | 
## |-------------------------|
## 
## ===============================================
##               Outcome
## Treatment     Conceived   Not Conceived   Total
## -----------------------------------------------
## Doxycyline            5              25      30
##                     4.7            25.3        
## -----------------------------------------------
## Placebo               4              24      28
##                     4.3            23.7        
## -----------------------------------------------
## Total                 9              49      58
## ===============================================</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( <span class="kw">table</span>(Conceived), <span class="dt">simulate.p.value=</span><span class="ot">TRUE</span>, <span class="dt">B=</span><span class="dv">10000</span> )</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 10000
##  replicates)
## 
## data:  table(Conceived)
## X-squared = 0.062628, df = NA, p-value = 1</code></pre>
</div>
<div id="rxc-tables" class="section level3">
<h3><span class="header-section-number">10.8.2</span> RxC tables</h3>
<p>We next expand this same analysis to consider cases where we have explantory variable with <span class="math inline">\(C\)</span> levels and the response variable has <span class="math inline">\(R\)</span> levels, and so the table of observations has <span class="math inline">\(R\)</span> rows and <span class="math inline">\(C\)</span> columns.</p>
<p>There was nothing special about the analysis that required only 2x2 tables. Exanding this the expected value for the <span class="math inline">\(i,j\)</span> cell in the table is still <span class="math display">\[E_{ij} = \frac{n_{i\cdot} n_{\cdot j}}{n}\]</span></p>
<p>As before we define the test statistic as <span class="math display">\[\begin{aligned}X^{2}  
  &amp;=    \sum_{\textrm{all ij cells}}\frac{\left(O_{ij}-E_{ij}\right)^{2}}{E_{ij}}
  \end{aligned}\]</span></p>
<p>If we have sufficient samples sizes in each cell (general rule-of-thumb is greater than 5 per cell), then we could compare this test statistic to a Chi-Squared distribution with <span class="math inline">\((R-1)(C-1)\)</span> degrees of freedom. <span class="math display">\[p.value = Pr( \chi_{(r-1)(c-1)} &gt; X^2 )\]</span></p>
<p>We consider some data from the American Community Survey, which is a survey administered by the US Census Bureau and given to approximately 3% of all US households. The package <code>Lock5Data</code> has a dataset, <code>ACS</code>, which is a subsample of <span class="math inline">\(n=1000\)</span> respondents of that 2010 survey. In particular, we want to examine the relationship between race and marriage status. In particular if white respondents are more likely to be married than asian or black (or other) races.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(ACS, <span class="dt">package=</span><span class="st">&#39;Lock5Data&#39;</span>)
ACS &lt;-<span class="st"> </span>ACS %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Married =</span> <span class="kw">ifelse</span>(Married==<span class="dv">1</span>, <span class="st">&#39;Married&#39;</span>,<span class="st">&#39;Single&#39;</span>))
tab &lt;-<span class="st"> </span>mosaic::<span class="kw">tally</span>(Married ~<span class="st"> </span>Race, <span class="dt">data=</span>ACS)
tab</code></pre></div>
<pre><code>##          Race
## Married   asian black other white
##   Married    37    25    20   355
##   Single     33    81    43   406</code></pre>
<p>Often I find it is difficult to really understand a table and find a good graph more insightful.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp &lt;-<span class="st"> </span>ACS %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Race, Married) %&gt;%<span class="st"> </span>
<span class="st">  </span>dplyr::<span class="kw">count</span>() %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(Race) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n/<span class="kw">sum</span>(n))
<span class="kw">ggplot</span>(temp, <span class="kw">aes</span>(<span class="dt">x=</span>Race, <span class="dt">y=</span>proportion, <span class="dt">fill=</span>Married)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat=</span><span class="st">&#39;identity&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-278-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Use the CrossTable function to generate the Expected Cell values</span>
descr::<span class="kw">CrossTable</span>(tab, <span class="dt">expected=</span><span class="ot">TRUE</span>, 
                  <span class="dt">prop.r=</span><span class="ot">FALSE</span>, <span class="dt">prop.c=</span><span class="ot">FALSE</span>, <span class="dt">prop.t=</span><span class="ot">FALSE</span>, <span class="dt">prop.chisq=</span><span class="ot">FALSE</span>)</code></pre></div>
<pre><code>##    Cell Contents 
## |-------------------------|
## |                       N | 
## |              Expected N | 
## |-------------------------|
## 
## ================================================
##            Race
## Married    asian   black   other   white   Total
## ------------------------------------------------
## Married       37      25      20     355     437
##             30.6    46.3    27.5   332.6        
## ------------------------------------------------
## Single        33      81      43     406     563
##             39.4    59.7    35.5   428.4        
## ------------------------------------------------
## Total         70     106      63     761    1000
## ================================================</code></pre>
<p>Because the cell counts are quite large, the asymptotic approximations should be fine. We will compare the test statistic against a Chi-squared distribution with <span class="math inline">\((2-1)(4-1)=1*3=3\)</span> degrees of freedom.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pchisq</span>(<span class="fl">26.168</span>, <span class="dt">df=</span><span class="dv">3</span>)</code></pre></div>
<pre><code>## [1] 8.795319e-06</code></pre>
<p>therefore <span class="math display">\[p.value = Pr( \chi^2_3 &gt; 26.168 ) = 8.795\textrm{e-}06\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( tab )</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  tab
## X-squared = 26.168, df = 3, p-value = 8.797e-06</code></pre>
<p>If we are worried about the sample size begin large enough, we could perform a permutation based test by repeatedly shuffling the <code>Race</code> labels calculating the test statistic and then comparing the observed test statistic <span class="math inline">\(X^2=26.168\)</span> to the permutation</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>( tab, <span class="dt">simulate.p.value=</span><span class="ot">TRUE</span>, <span class="dt">B=</span><span class="dv">100000</span> )</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with simulated p-value (based on 1e+05
##  replicates)
## 
## data:  tab
## X-squared = 26.168, df = NA, p-value = 2e-05</code></pre>
<p>With such a small p-value, we know that we are unlikey to have observed such a large difference in marriage rates among our different races. It appears that white respondents are much more likely to be married than the other races listed, but is there a difference in rates between blacks and asians? What about asian and other?</p>
<p>Just as we wanted to perform an analysis on all pairwise comparisons amongst levels in an ANOVA analysis and control the overall Type I error rate, we will do the same thing but now using the Chi-squared test.</p>
<p>Conceptually we will just perform all possible pairwise tests and then adjust the resulting p-values to control for the number of comparisons.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We will use a function from the package fifer</span>
<span class="co"># chisq.post.hoc  wants the table arranged with the </span>
<span class="co"># factor you want to pairwise contasts as the rows</span>
<span class="kw">t</span>( tab )   <span class="co"># transpose the rows and columns</span></code></pre></div>
<pre><code>##        Married
## Race    Married Single
##   asian      37     33
##   black      25     81
##   other      20     43
##   white     355    406</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">result &lt;-<span class="st"> </span>fifer::<span class="kw">chisq.post.hoc</span>( <span class="kw">t</span>(tab),
    <span class="dt">test=</span>chisq.test, <span class="dt">simulate.p.value=</span><span class="ot">TRUE</span>, <span class="dt">B=</span><span class="dv">10000</span> ) </code></pre></div>
<pre><code>## Adjusted p-values used the fdr method.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">result</code></pre></div>
<pre><code>##        comparison  raw.p  adj.p
## 1 asian vs. black 0.0001 0.0003
## 2 asian vs. other 0.0202 0.0366
## 3 asian vs. white 0.3856 0.3856
## 4 black vs. other 0.2758 0.3309
## 5 black vs. white 0.0001 0.0003
## 6 other vs. white 0.0244 0.0366</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We really want to get the compact letter display for </span>
<span class="co"># graphing purposes, but there is some annoying details</span>
<span class="co"># that we really ought not have to think about.  So lets</span>
<span class="co"># make a little function to handle this stuff.</span>
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(stringr)
<span class="kw">library</span>(multcompView)
cld.chisq.post.hoc &lt;-<span class="st"> </span>function( obj, name ){
  p.values &lt;-<span class="st"> </span>obj$adj.p                <span class="co"># just the p-values</span>
  contrasts &lt;-<span class="st"> </span>obj$comparison %&gt;%
<span class="st">    </span><span class="kw">as.character</span>( ) %&gt;%
<span class="st">    </span><span class="kw">str_replace</span>( <span class="dt">pattern=</span><span class="kw">fixed</span>(<span class="st">&#39; vs. &#39;</span>), <span class="dt">replacement =</span> <span class="kw">fixed</span>(<span class="st">&#39;-&#39;</span>))
  <span class="kw">names</span>(p.values) &lt;-<span class="st"> </span>contrasts

  <span class="co"># finally we can pass p.values into a letters function</span>
  my.letters &lt;-<span class="st"> </span>multcompView::<span class="kw">multcompLetters</span>(p.values)
  letter.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">TEMP=</span><span class="kw">names</span>(my.letters$Letters), 
                           <span class="dt">.group =</span> my.letters$Letters ) 
  <span class="kw">colnames</span>(letter.df)[<span class="dv">1</span>] &lt;-<span class="st"> </span>name
  <span class="kw">rownames</span>(letter.df) &lt;-<span class="st"> </span><span class="ot">NULL</span>
  <span class="kw">return</span>(letter.df)
}  </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cld.chisq.post.hoc</span>(result, <span class="st">&#39;Race&#39;</span>)</code></pre></div>
<pre><code>##    Race .group
## 1 asian      a
## 2 black      b
## 3 other      b
## 4 white      a</code></pre>
<p>There are a number of other questions that I might consider, such as confidence intervals for the proportions married in each race. However, those questions require a few more assumptions about the structure of the data and will be addressed when we study logistic regression.</p>
</div>
</div>
<div id="exercises-10" class="section level2">
<h2><span class="header-section-number">10.9</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Is gender independent of education level? A random sample of <span class="math inline">\(n=395\)</span> people were surveyed and each person was asked to report the highest education level they obtained. The data that resulted from the survey is summarized in the following table:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><strong>High School</strong></th>
<th align="center"><strong>Bachelors</strong></th>
<th align="center"><strong>Masters</strong></th>
<th align="center"><strong>Ph.d.</strong></th>
<th align="center"><strong>Total</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Female</strong></td>
<td align="center">60</td>
<td align="center">54</td>
<td align="center">46</td>
<td align="center">41</td>
<td align="center">201</td>
</tr>
<tr class="even">
<td align="center"><strong>Male</strong></td>
<td align="center">40</td>
<td align="center">44</td>
<td align="center">53</td>
<td align="center">57</td>
<td align="center">194</td>
</tr>
<tr class="odd">
<td align="center"><strong>Total</strong></td>
<td align="center">100</td>
<td align="center">98</td>
<td align="center">99</td>
<td align="center">98</td>
<td align="center">395</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Calculate the expected cell counts for each Gender and Degree combination.</li>
<li>Calculate the <span class="math inline">\(X^2\)</span> test statistic.</li>
<li>Calculate the appropriate p-value using the asymptotic approximation.</li>
</ol></li>
<li><p>We consider some data from the American Community Survey, which is a survey administered by the US Census Bureau and given to approximately 3% of all US households. The package <code>Lock5Data</code> has a dataset, <code>ACS</code>, which is a subsample of <span class="math inline">\(n=1000\)</span> respondents of that 2010 survey. In particular, we want to examine the relationship between race and having health insurance.</p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp &lt;-<span class="st"> </span>ACS %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">HealthInsurance =</span> <span class="kw">factor</span>(<span class="kw">ifelse</span>(HealthInsurance ==<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;Have&quot;</span>,<span class="st">&quot;None&quot;</span>))) %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(Race, HealthInsurance) %&gt;%<span class="st"> </span>
<span class="st">  </span>dplyr::<span class="kw">count</span>() %&gt;%
<span class="st">  </span><span class="kw">group_by</span>(Race) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proportion =</span> n/<span class="kw">sum</span>(n))
<span class="kw">ggplot</span>(temp, <span class="kw">aes</span>(<span class="dt">x=</span>Race, <span class="dt">y=</span>proportion, <span class="dt">fill=</span>HealthInsurance)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat=</span><span class="st">&#39;identity&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_I_files/figure-html/unnamed-chunk-285-1.png" width="672" /> a) Generate a table summarizing how many respondents of each race has health insurance. b) Test the hypothesis that there is no association between race and having health insurance. using both the asymptotic method and the permutation method. Is your inference the same in both cases? c) Establish which racial groups are different in the proportion of respondents that have health insurance.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="9-analysis-of-variance-anova.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/570/raw/master/10_Regression.Rmd",
"text": "Edit"
},
"download": [["Statistical_Methods_I.pdf", "PDF"], ["Statistical_Methods_I.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
